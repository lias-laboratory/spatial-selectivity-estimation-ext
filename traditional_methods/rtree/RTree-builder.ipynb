{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata for 14 datasets\n",
      "Building R-trees for 14 datasets\n",
      "\n",
      "================================================================================\n",
      "DATASET 1/14: yago2\n",
      "================================================================================\n",
      "Building R-tree for yago2...\n",
      "R-tree model already exists for yago2\n",
      "Finished processing yago2 (1/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 2/14: craftwaysorted\n",
      "================================================================================\n",
      "Building R-tree for craftwaysorted...\n",
      "R-tree model already exists for craftwaysorted\n",
      "Finished processing craftwaysorted (2/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 3/14: zcta5\n",
      "================================================================================\n",
      "Building R-tree for zcta5...\n",
      "R-tree model already exists for zcta5\n",
      "Finished processing zcta5 (3/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 4/14: areawater\n",
      "================================================================================\n",
      "Building R-tree for areawater...\n",
      "R-tree model already exists for areawater\n",
      "Finished processing areawater (4/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 5/14: aerowaythingnodesorted\n",
      "================================================================================\n",
      "Building R-tree for aerowaythingnodesorted...\n",
      "R-tree model already exists for aerowaythingnodesorted\n",
      "Finished processing aerowaythingnodesorted (5/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 6/14: emergencythingwaysorted\n",
      "================================================================================\n",
      "Building R-tree for emergencythingwaysorted...\n",
      "R-tree model already exists for emergencythingwaysorted\n",
      "Finished processing emergencythingwaysorted (6/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 7/14: historicthingwaysorted\n",
      "================================================================================\n",
      "Building R-tree for historicthingwaysorted...\n",
      "R-tree model already exists for historicthingwaysorted\n",
      "Finished processing historicthingwaysorted (7/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 8/14: aerowaythingwaysorted\n",
      "================================================================================\n",
      "Building R-tree for aerowaythingwaysorted...\n",
      "R-tree model already exists for aerowaythingwaysorted\n",
      "Finished processing aerowaythingwaysorted (8/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 9/14: cyclewaythingwaysorted\n",
      "================================================================================\n",
      "Building R-tree for cyclewaythingwaysorted...\n",
      "R-tree model already exists for cyclewaythingwaysorted\n",
      "Finished processing cyclewaythingwaysorted (9/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 10/14: powerthingwaysorted\n",
      "================================================================================\n",
      "Building R-tree for powerthingwaysorted...\n",
      "R-tree model already exists for powerthingwaysorted\n",
      "Finished processing powerthingwaysorted (10/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 11/14: leisurewaysorted\n",
      "================================================================================\n",
      "Building R-tree for leisurewaysorted...\n",
      "Loading 29382860 objects for leisurewaysorted...\n",
      "  100.0% loaded... (valid: 29382807, points: 26827770, invalid: 53)\n",
      "R-tree building complete in 3285.01 seconds. Valid objects: 29382807 (including 26827770 points), Invalid geometries: 53\n",
      "Generating level nodes for leisurewaysorted...\n",
      "Using 54x54 grid to generate level nodes\n",
      "Generated 687 level nodes for leisurewaysorted\n",
      "R-tree built successfully for leisurewaysorted\n",
      "Model size: 1866.94 MB\n",
      "Estimated memory usage: 1232.95 MB\n",
      "Number of level nodes: 687\n",
      "Finished processing leisurewaysorted (11/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 12/14: barrierthingwaysorted\n",
      "================================================================================\n",
      "Building R-tree for barrierthingwaysorted...\n",
      "R-tree model already exists for barrierthingwaysorted\n",
      "Finished processing barrierthingwaysorted (12/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 13/14: powerthingnodesorted\n",
      "================================================================================\n",
      "Building R-tree for powerthingnodesorted...\n",
      "R-tree model already exists for powerthingnodesorted\n",
      "Finished processing powerthingnodesorted (13/14)\n",
      "\n",
      "================================================================================\n",
      "DATASET 14/14: arealm\n",
      "================================================================================\n",
      "Building R-tree for arealm...\n",
      "R-tree model already exists for arealm\n",
      "Finished processing arealm (14/14)\n",
      "\n",
      "Saved metadata for all R-trees\n",
      "All R-trees built and saved!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "import re\n",
    "import configparser\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from rtree import index\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RTreeBuilder:\n",
    "    def __init__(self, data_dir=\"../../large_files\"):\n",
    "        \"\"\"\n",
    "        Build and save R-trees for spatial datasets\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_dir : str\n",
    "            Directory for storing R-tree models\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.universe_boundaries = {}\n",
    "        self.dataset_sizes = {}\n",
    "        \n",
    "        # Create directories for storing models\n",
    "        self.rtree_dir = f\"{data_dir}/traditional_methods/rtree/models\"\n",
    "        os.makedirs(self.rtree_dir, exist_ok=True)\n",
    "        \n",
    "        # Load dataset metadata\n",
    "        self.load_spatial_statistics()\n",
    "        \n",
    "        # R-tree parameters - FIXED the variant specification\n",
    "        self.rtree_params = {\n",
    "            'leaf_capacity': 100,\n",
    "            'fill_factor': 0.8,\n",
    "            'near_minimum_overlap_factor': 32,\n",
    "            'variant': 'RT_Star',  # Fixed: no need for rtree.index prefix\n",
    "            'dimension': 2\n",
    "        }\n",
    "    \n",
    "    def load_spatial_statistics(self):\n",
    "        \"\"\"Load dataset information from spatial_statistics.csv\"\"\"\n",
    "        try:\n",
    "            stats_df = pd.read_csv(\"../../spatial_statistics.csv\")\n",
    "            for _, row in stats_df.iterrows():\n",
    "                table_name = row['Table Name']\n",
    "                total_objects = row['Total Spatial Objects']\n",
    "                bbox_str = row['Universe Limits (Bounding Box)']\n",
    "                \n",
    "                # Parse bounding box\n",
    "                bbox = self.parse_bbox(bbox_str)\n",
    "                self.universe_boundaries[table_name] = bbox\n",
    "                self.dataset_sizes[table_name] = int(total_objects)\n",
    "                \n",
    "            print(f\"Loaded metadata for {len(self.universe_boundaries)} datasets\")\n",
    "            sys.stdout.flush()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading spatial statistics: {e}\")\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    def parse_bbox(self, bbox_str):\n",
    "        \"\"\"Parse bounding box string into coordinates\"\"\"\n",
    "        pattern = r\"BOX\\(([-\\d\\.]+) ([-\\d\\.]+),([-\\d\\.]+) ([-\\d\\.]+)\\)\"\n",
    "        match = re.search(pattern, bbox_str)\n",
    "        if match:\n",
    "            xmin = float(match.group(1))\n",
    "            ymin = float(match.group(2))\n",
    "            xmax = float(match.group(3))\n",
    "            ymax = float(match.group(4))\n",
    "            return (xmin, ymin, xmax, ymax)\n",
    "        return (-180, -90, 180, 90)  # Default if parsing fails\n",
    "    \n",
    "    def connect_to_database(self):\n",
    "        \"\"\"Connect to the database containing spatial data\"\"\"\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(\"../../dataset_generation/config.ini\")\n",
    "        db_params = config[\"database\"]\n",
    "        \n",
    "        try:\n",
    "            conn = psycopg2.connect(\n",
    "                dbname=db_params[\"dbname\"],\n",
    "                user=db_params[\"user\"],\n",
    "                password=db_params[\"password\"],\n",
    "                host=db_params[\"host\"],\n",
    "                port=db_params[\"port\"]\n",
    "            )\n",
    "            return conn\n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"Database connection error: {e}\")\n",
    "            sys.stdout.flush()\n",
    "            return None\n",
    "    \n",
    "    def build_rtree(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Build an R-tree index for a dataset and save to disk\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dataset_name : str\n",
    "            Name of the dataset to build R-tree for\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Model metadata including size and parameters\n",
    "        \"\"\"\n",
    "        print(f\"Building R-tree for {dataset_name}...\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Check if dataset exists\n",
    "        if dataset_name not in self.universe_boundaries:\n",
    "            raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "        \n",
    "        # Output paths\n",
    "        rtree_path = f\"{self.rtree_dir}/{dataset_name}_rtree\"\n",
    "        level_nodes_path = f\"{self.rtree_dir}/{dataset_name}_level_nodes.pkl\"\n",
    "        metadata_path = f\"{self.rtree_dir}/{dataset_name}_metadata.json\"\n",
    "        \n",
    "        # Check if model already exists\n",
    "        if os.path.exists(f\"{rtree_path}.dat\") and os.path.exists(level_nodes_path):\n",
    "            print(f\"R-tree model already exists for {dataset_name}\")\n",
    "            if os.path.exists(metadata_path):\n",
    "                return pd.read_json(metadata_path, typ='series').to_dict()\n",
    "            return None\n",
    "        \n",
    "        # Define the properties for the R-tree\n",
    "        p = index.Property()\n",
    "        p.dimension = self.rtree_params['dimension']\n",
    "        p.leaf_capacity = self.rtree_params['leaf_capacity']\n",
    "        p.fill_factor = self.rtree_params['fill_factor']\n",
    "        p.near_minimum_overlap_factor = self.rtree_params['near_minimum_overlap_factor']\n",
    "        \n",
    "        # FIXED: Set variant properly using the imported index module\n",
    "        if self.rtree_params['variant'] == 'RT_Star':\n",
    "            p.variant = index.RT_Star\n",
    "        elif self.rtree_params['variant'] == 'RT_Linear':\n",
    "            p.variant = index.RT_Linear\n",
    "        elif self.rtree_params['variant'] == 'RT_Quadratic':\n",
    "            p.variant = index.RT_Quadratic\n",
    "        else:\n",
    "            p.variant = index.RT_Star  # Default to RT_Star\n",
    "            \n",
    "        p.tight_mbr = True\n",
    "        \n",
    "        # Create the R-tree with disk storage\n",
    "        idx = index.Index(rtree_path, properties=p)\n",
    "        \n",
    "        # Store objects for level analysis\n",
    "        objects = {}\n",
    "        \n",
    "        try:\n",
    "            # Connect to database and load objects\n",
    "            conn = self.connect_to_database()\n",
    "            if not conn:\n",
    "                raise ValueError(f\"Failed to connect to database for {dataset_name}\")\n",
    "                \n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Query to get objects with their MBRs\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT id, ST_XMin(geometry), ST_YMin(geometry), \n",
    "                    ST_XMax(geometry), ST_YMax(geometry)\n",
    "                FROM {dataset_name}_mbr\n",
    "            \"\"\")\n",
    "            \n",
    "            # Process in batches\n",
    "            batch_size = 10000\n",
    "            total_objects = self.dataset_sizes[dataset_name]\n",
    "            total_processed = 0\n",
    "            valid_objects = 0\n",
    "            point_objects = 0\n",
    "            invalid_objects = 0\n",
    "            \n",
    "            print(f\"Loading {total_objects} objects for {dataset_name}...\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # Use a progress counter\n",
    "            progress_step = max(1, total_objects // 10)\n",
    "            \n",
    "            # Small buffer to add to point geometries\n",
    "            POINT_BUFFER = 0.000001  # Small epsilon for point geometries\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            while True:\n",
    "                batch_data = cursor.fetchmany(batch_size)\n",
    "                if not batch_data:\n",
    "                    break\n",
    "                    \n",
    "                for obj in batch_data:\n",
    "                    obj_id, xmin, ymin, xmax, ymax = obj\n",
    "                    \n",
    "                    # Skip null geometries\n",
    "                    if xmin is None or ymin is None or xmax is None or ymax is None:\n",
    "                        invalid_objects += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Handle point geometries (where min = max)\n",
    "                    is_point = False\n",
    "                    if xmin == xmax and ymin == ymax:\n",
    "                        is_point = True\n",
    "                        point_objects += 1\n",
    "                        # Add a small buffer to make it a valid rectangle for R-tree\n",
    "                        xmin -= POINT_BUFFER\n",
    "                        ymin -= POINT_BUFFER\n",
    "                        xmax += POINT_BUFFER\n",
    "                        ymax += POINT_BUFFER\n",
    "                    \n",
    "                    # Handle invalid geometries where min > max\n",
    "                    if xmin > xmax:\n",
    "                        mid = (xmin + xmax) / 2\n",
    "                        xmin, xmax = mid - POINT_BUFFER, mid + POINT_BUFFER\n",
    "                    \n",
    "                    if ymin > ymax:\n",
    "                        mid = (ymin + ymax) / 2\n",
    "                        ymin, ymax = mid - POINT_BUFFER, mid + POINT_BUFFER\n",
    "                    \n",
    "                    # Final validation\n",
    "                    if xmin >= xmax or ymin >= ymax:\n",
    "                        invalid_objects += 1\n",
    "                        continue\n",
    "                        \n",
    "                    # Insert into R-tree\n",
    "                    try:\n",
    "                        idx.insert(obj_id, (xmin, ymin, xmax, ymax))\n",
    "                        \n",
    "                        # Store in objects dictionary (for level analysis)\n",
    "                        objects[obj_id] = (xmin, ymin, xmax, ymax)\n",
    "                        valid_objects += 1\n",
    "                    except Exception as e:\n",
    "                        invalid_objects += 1\n",
    "                        continue\n",
    "                \n",
    "                total_processed += len(batch_data)\n",
    "                \n",
    "                # Simple progress reporting\n",
    "                if total_processed % progress_step == 0:\n",
    "                    print(f\"  {min(total_processed, total_objects) / total_objects * 100:.1f}% loaded... \"\n",
    "                        f\"(valid: {valid_objects}, points: {point_objects}, invalid: {invalid_objects})\", \n",
    "                        end=\"\\r\", flush=True)\n",
    "                    sys.stdout.flush()\n",
    "            \n",
    "            build_time = time.time() - start_time\n",
    "            print(f\"\\nR-tree building complete in {build_time:.2f} seconds. \"\n",
    "                  f\"Valid objects: {valid_objects} (including {point_objects} points), \"\n",
    "                  f\"Invalid geometries: {invalid_objects}\")\n",
    "            sys.stdout.flush()\n",
    "            conn.close()\n",
    "            \n",
    "            # Check if we have enough valid objects\n",
    "            if valid_objects == 0:\n",
    "                print(f\"No valid objects found for {dataset_name}. Using fallback mechanism.\")\n",
    "                level_nodes = self.create_fallback_nodes(dataset_name)\n",
    "            else:\n",
    "                # Generate level statistics for estimation\n",
    "                level_nodes = self.generate_level_nodes(dataset_name, idx, objects, self.rtree_params['leaf_capacity'])\n",
    "            \n",
    "            # Save level nodes to disk\n",
    "            with open(level_nodes_path, 'wb') as f:\n",
    "                pickle.dump(level_nodes, f)\n",
    "            \n",
    "            # Calculate model size\n",
    "            model_size_disk = 0\n",
    "            if os.path.exists(f\"{rtree_path}.dat\"):\n",
    "                model_size_disk += os.path.getsize(f\"{rtree_path}.dat\")\n",
    "            if os.path.exists(f\"{rtree_path}.idx\"):\n",
    "                model_size_disk += os.path.getsize(f\"{rtree_path}.idx\")\n",
    "                \n",
    "            level_nodes_size = os.path.getsize(level_nodes_path)\n",
    "            \n",
    "            # Calculate memory usage estimate\n",
    "            memory_per_entry = 44  # Bytes per entry (2 coordinates * 2 * 8 bytes + indexing overhead)\n",
    "            memory_usage = valid_objects * memory_per_entry\n",
    "            \n",
    "            # Compute number of nodes at each level (estimated)\n",
    "            est_leaf_nodes = np.ceil(valid_objects / self.rtree_params['leaf_capacity'])\n",
    "            leaf_level = int(np.ceil(np.log(max(1, est_leaf_nodes)) / \n",
    "                                    np.log(max(2, self.rtree_params['leaf_capacity']))))\n",
    "            \n",
    "            # Create detailed model metadata\n",
    "            metadata = {\n",
    "                'dataset': dataset_name,\n",
    "                'total_objects': total_objects,\n",
    "                'valid_objects': valid_objects,\n",
    "                'point_objects': point_objects,\n",
    "                'invalid_objects': invalid_objects,\n",
    "                'model_size_bytes': model_size_disk,\n",
    "                'level_nodes_size_bytes': level_nodes_size,\n",
    "                'total_size_bytes': model_size_disk + level_nodes_size,\n",
    "                'estimated_memory_bytes': memory_usage,\n",
    "                'estimated_levels': leaf_level + 1,\n",
    "                'build_time_seconds': build_time,\n",
    "                'rtree_params': self.rtree_params,\n",
    "                'num_level_nodes': len(level_nodes),\n",
    "                'grid_size': int(np.sqrt(len(level_nodes))) if len(level_nodes) > 0 else 0\n",
    "            }\n",
    "            \n",
    "            # Save metadata\n",
    "            pd.Series(metadata).to_json(metadata_path)\n",
    "            \n",
    "            print(f\"R-tree built successfully for {dataset_name}\")\n",
    "            print(f\"Model size: {metadata['total_size_bytes'] / (1024*1024):.2f} MB\")\n",
    "            print(f\"Estimated memory usage: {metadata['estimated_memory_bytes'] / (1024*1024):.2f} MB\")\n",
    "            print(f\"Number of level nodes: {metadata['num_level_nodes']}\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            return metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error building R-tree for {dataset_name}: {e}\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # Create fallback\n",
    "            level_nodes = self.create_fallback_nodes(dataset_name)\n",
    "            with open(level_nodes_path, 'wb') as f:\n",
    "                pickle.dump(level_nodes, f)\n",
    "            \n",
    "            return None\n",
    "    \n",
    "    def create_fallback_nodes(self, dataset_name):\n",
    "        \"\"\"Create simple grid nodes when R-tree building fails\"\"\"\n",
    "        print(f\"Creating fallback estimation nodes for {dataset_name}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        if dataset_name not in self.universe_boundaries:\n",
    "            return []\n",
    "        \n",
    "        universe = self.universe_boundaries[dataset_name]\n",
    "        total_objects = self.dataset_sizes[dataset_name]\n",
    "        \n",
    "        # Create a simple 4x4 grid of nodes covering the universe\n",
    "        xmin, ymin, xmax, ymax = universe\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        if width <= 0 or height <= 0:\n",
    "            # Use default values if universe is invalid\n",
    "            xmin, ymin = -180, -90\n",
    "            xmax, ymax = 180, 90\n",
    "            width = 360\n",
    "            height = 180\n",
    "        \n",
    "        grid_size = 4\n",
    "        cell_width = width / grid_size\n",
    "        cell_height = height / grid_size\n",
    "        nodes = []\n",
    "        \n",
    "        # Create a uniform grid\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                cell_xmin = xmin + i * cell_width\n",
    "                cell_ymin = ymin + j * cell_height\n",
    "                cell_xmax = cell_xmin + cell_width\n",
    "                cell_ymax = cell_ymin + cell_height\n",
    "                \n",
    "                # Assume uniform distribution\n",
    "                estimated_objects = total_objects / (grid_size * grid_size)\n",
    "                \n",
    "                nodes.append({\n",
    "                    'mbr': (cell_xmin, cell_ymin, cell_xmax, cell_ymax),\n",
    "                    'objects': estimated_objects,\n",
    "                    'area': cell_width * cell_height\n",
    "                })\n",
    "        \n",
    "        print(f\"Created {len(nodes)} fallback nodes for {dataset_name}\")\n",
    "        sys.stdout.flush()\n",
    "        return nodes\n",
    "    \n",
    "    def generate_level_nodes(self, dataset_name, idx, objects, leaf_capacity):\n",
    "        \"\"\"\n",
    "        Generate nodes at the level before leaves using grid-based spatial clustering\n",
    "        \"\"\"\n",
    "        print(f\"Generating level nodes for {dataset_name}...\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        universe = self.universe_boundaries[dataset_name]\n",
    "        total_objects = len(objects)\n",
    "        \n",
    "        # If we have very few objects, just use the universe as one node\n",
    "        if total_objects <= leaf_capacity * 2:\n",
    "            return [{\n",
    "                'mbr': universe,\n",
    "                'objects': total_objects,\n",
    "                'area': (universe[2] - universe[0]) * (universe[3] - universe[1])\n",
    "            }]\n",
    "        \n",
    "        # Calculate grid size based on heuristic\n",
    "        est_leaf_nodes = np.ceil(total_objects / leaf_capacity)\n",
    "        est_parent_nodes = np.ceil(est_leaf_nodes / leaf_capacity)\n",
    "        grid_size = max(2, int(np.sqrt(est_parent_nodes)))\n",
    "        \n",
    "        print(f\"Using {grid_size}x{grid_size} grid to generate level nodes\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        univ_xmin, univ_ymin, univ_xmax, univ_ymax = universe\n",
    "        grid_width = (univ_xmax - univ_xmin) / grid_size\n",
    "        grid_height = (univ_ymax - univ_ymin) / grid_size\n",
    "        \n",
    "        # First pass: find density of each grid cell\n",
    "        grid_counts = np.zeros((grid_size, grid_size))\n",
    "        object_count = 0\n",
    "        \n",
    "        # Sample a subset of objects for better performance\n",
    "        sample_size = min(50000, total_objects)\n",
    "        sample_ids = list(objects.keys())\n",
    "        if total_objects > sample_size:\n",
    "            sample_ids = np.random.choice(sample_ids, sample_size, replace=False)\n",
    "        \n",
    "        for obj_id in sample_ids:\n",
    "            obj = objects[obj_id]\n",
    "            xmin, ymin, xmax, ymax = obj\n",
    "            \n",
    "            # Use center point for grid assignment\n",
    "            x_center = (xmin + xmax) / 2\n",
    "            y_center = (ymin + ymax) / 2\n",
    "            \n",
    "            # Skip objects outside universe\n",
    "            if (x_center < univ_xmin or x_center >= univ_xmax or\n",
    "                y_center < univ_ymin or y_center >= univ_ymax):\n",
    "                continue\n",
    "                \n",
    "            # Calculate grid indices\n",
    "            grid_x = min(grid_size - 1, int((x_center - univ_xmin) / grid_width))\n",
    "            grid_y = min(grid_size - 1, int((y_center - univ_ymin) / grid_height))\n",
    "            \n",
    "            # Increment count\n",
    "            grid_counts[grid_x, grid_y] += 1\n",
    "            object_count += 1\n",
    "        \n",
    "        # Convert counts to density factors\n",
    "        if object_count > 0:\n",
    "            grid_counts = grid_counts / object_count * total_objects\n",
    "        \n",
    "        # Second pass: create nodes based on grid cells\n",
    "        nodes = []\n",
    "        \n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                # Calculate node MBR\n",
    "                node_xmin = univ_xmin + i * grid_width\n",
    "                node_ymin = univ_ymin + j * grid_height\n",
    "                node_xmax = node_xmin + grid_width\n",
    "                node_ymax = node_ymin + grid_height\n",
    "                \n",
    "                # Calculate node statistics\n",
    "                node_mbr = (node_xmin, node_ymin, node_xmax, node_ymax)\n",
    "                node_area = grid_width * grid_height\n",
    "                estimated_objects = int(grid_counts[i, j])\n",
    "                \n",
    "                # Create node if it has objects\n",
    "                if estimated_objects > 0:\n",
    "                    nodes.append({\n",
    "                        'mbr': node_mbr,\n",
    "                        'objects': estimated_objects,\n",
    "                        'area': node_area\n",
    "                    })\n",
    "        \n",
    "        print(f\"Generated {len(nodes)} level nodes for {dataset_name}\")\n",
    "        sys.stdout.flush()\n",
    "        return nodes\n",
    "    \n",
    "    def build_all_rtrees(self):\n",
    "        \"\"\"Build and save R-trees for all datasets\"\"\"\n",
    "        dataset_names = list(self.universe_boundaries.keys())\n",
    "        \n",
    "        print(f\"Building R-trees for {len(dataset_names)} datasets\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        all_metadata = []\n",
    "        \n",
    "        for idx, dataset_name in enumerate(dataset_names, start=1):\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"DATASET {idx}/{len(dataset_names)}: {dataset_name}\")\n",
    "            print(\"=\"*80)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            try:\n",
    "                metadata = self.build_rtree(dataset_name)\n",
    "                if metadata:\n",
    "                    all_metadata.append(metadata)\n",
    "                print(f\"Finished processing {dataset_name} ({idx}/{len(dataset_names)})\")\n",
    "                sys.stdout.flush()\n",
    "            except Exception as e:\n",
    "                print(f\"Error building R-tree for {dataset_name}: {e}\")\n",
    "                print(\"Moving to next dataset\")\n",
    "                sys.stdout.flush()\n",
    "        \n",
    "        # Save combined metadata\n",
    "        if all_metadata:\n",
    "            pd.DataFrame(all_metadata).to_csv(f\"{self.rtree_dir}/all_rtree_metadata.csv\", index=False)\n",
    "            print(\"\\nSaved metadata for all R-trees\")\n",
    "        else:\n",
    "            print(\"No R-trees were built successfully\")\n",
    "        \n",
    "        return all_metadata\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    builder = RTreeBuilder()\n",
    "    builder.build_all_rtrees()\n",
    "    print(\"All R-trees built and saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
